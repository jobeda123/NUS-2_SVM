{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Window_size=23.ipynb","provenance":[],"collapsed_sections":["qWKecSMGvKBY","xjD8SrYTvKC6","pWJz9dxovKDL","xt8Ntri6vKDN","_-Y87jWRvKDQ"]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"OqLf9o29vKBU"},"source":["############ Import libraries\n","\n","import pandas as pd\n","import numpy as np\n","from pandas import read_csv\n","\n","\n","import random\n","import imblearn\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from collections import Counter\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import ClusterCentroids \n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","\n","from xgboost import XGBClassifier\n","from sklearn import tree\n","import seaborn as sns\n","from sklearn.svm import SVC\n","from sklearn import svm\n","import lightgbm as lgb\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qWKecSMGvKBY"},"source":["# Data Pre-processing"]},{"cell_type":"code","metadata":{"id":"QSKL9MDkvKBY"},"source":["## Read Dataset\n","\n","df = pd.read_table(\"Glycation.elm\")\n","#print(df.head())\n","print(len(df))\n","\n","\n","## Table Split\n","\n","table1= df.iloc[:,[0,2]]\n","table2 =df.iloc[:,[0,4]]\n","\n","\n","#print(table2.iloc[0,0])\n","\n","#print(table1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVMexzXkvKBb"},"source":["## convert to csv file for table1\n","\n","tb1=pd.DataFrame({'PLMD ID':table1.iloc[:,0],'Position':table1.iloc[:,1] })\n","filename= 'table1.csv'\n","tb1.to_csv(filename,index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9Pz6rQKvKBd"},"source":["## convert to csv file for table 2\n","\n","tb2=pd.DataFrame({'PLMD ID':table2.iloc[:,0],'Sequence':table2.iloc[:,1] })\n","filename= 'table2.csv'\n","tb2.to_csv(filename,index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsaHvP3pvKBi"},"source":["## Convert To fasta(File Write)\n","\n","dataWrite = open(\"BeforeCD-HIT.fasta\",\"w\")\n","l = len(table2)\n","#print(l)\n","for i in range (l):\n","  dataWrite.write(\">\" + table2.iloc[i,0] + \"\\n\" + table2.iloc[i,1]+\"\\n\")\n","\n","dataWrite.close()\n","#print(dataWrite.read())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3J0JZUzvKBl"},"source":["##### Convert fasta to CSV\n","\n","dataRead = open('1593951336.fas.1','r')\n","v=[]\n","#print(dataRead.read())\n","\n","for line in dataRead:\n","  v.append(line)\n","\n","\n","#print(len(v))\n","pid=[]\n","seq=[]\n","i=0\n","\n","for i in range(len(v)):\n","  if(i%2==0):\n","    pid.append(v[i])\n","\n","for i in range(len(v)):\n","  if(i%2!=0):\n","    seq.append(v[i])\n","\n","#print(len(seq))\n","#print(pid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZjsKzeIvKBn"},"source":["###  Remove \">\" this sign from pid\n","\n","x=len(pid)\n","#print(pid)\n","for i in range(x):\n","  t=pid[i]\n","  t=t.replace(\">\",\"\")\n","  t=t.replace(\"\\n\",\"\")\n","  pid[i]=t\n","\n","#print(pid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaVfnZ0vvKBq"},"source":["###  Convert CSV file after CD-HIT\n","\n","f=pd.DataFrame({'PLMD ID':pid,'Sequence':seq })\n","filename= 'afterCD-HIT.csv'\n","f.to_csv(filename,index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0G3drW3vKBs"},"source":["##### For Finding All Peptides\n","\n","\n","out= pd.read_csv('afterCD-HIT.csv')\n","\n","seq=out.iloc[:,1]\n","\n","window_size=23                         ##### for window size\n","half=int(np.floor(window_size/2))      #### half of window size\n","\n","print(\"*****************  Here Window Size is:\",window_size)\n","\n","s_len=len(seq)\n","#print(\"SEQ lenght: \",s_len)\n","\n","all_pep=[]\n","\n","c=0\n","for i in range(s_len):\n","  pep=seq[i]\n","  #pep.rstrip(\"\\n\")\n","  pep.replace(\"\\n\",\"\")\n","  #print(\"Full Seq: \",pep)\n","  singleSeq_len=len(pep)\n","  #print(\"Single Seq Length: \",singleSeq_len)\n","  for j in range(singleSeq_len):\n","    if( pep[j] == \"K\" ):\n","      c+=1\n","      s=j-1         ### s mane start\n","      e=j+half       ### e mane end\n","\n","      #print(\"Start, end,position,strLen: \",s,e,j,singleSeq_len)\n","      \n","      if( s>=half and e<(singleSeq_len-1) ):    ### Sob thik ase\n","        s_l=j-half\n","        f=pep[s_l:e+1]\n","        all_pep.append(f)\n","        #print(\"All Peptides: \",f)\n","        #if( len(f) < 23 ):\n","         #       print(\"Less 23\")\n","        #else:\n","         # print(\"Length okay from sob thik\")\n","      \n","\n","      elif(s>=half and e>=(singleSeq_len-2)):\n","        f_len=len(pep)-1\n","        #print(\"Len: \",f_len)\n","        #print(\"********Right e problem\")\n","        #print(\"SEQ: \",pep)\n","        s_l=j-half\n","        f=pep[s_l:f_len]\n","        #print(\"JHDHD: \",f,len(f))\n","        r=window_size-len(f)\n","        #print(\"Need: \",r)\n","        for h in range(r):\n","          f = f + \"X\"\n","        \n","        #print(\"After adding X: \",f)\n","        all_pep.append(f)\n","        #if( len(f) < 23 ):\n","          #      print(\"Less 23\")\n","        #else:\n","         # print(\"Length okay from right \")\n","\n","\n","      elif( s<half and e<=singleSeq_len):\n","        f_len=len(pep)\n","        #print(\"******First e problem\")\n","        #print(\"start, end, pos,len:\",s,e,j,f_len)\n","        f=pep[0:e+1]\n","        #print(\"Before adding X: \",f)\n","        r=half-(s+1)\n","        #print(\"Need X: \",r)\n","        \n","        for q in range(r):\n","          f=\"X\"+f\n","        #print(\"After adding X: \",f)\n","        all_pep.append(f)\n","        #if( len(f) < 23 ):\n","         #       print(\"Less 23\")\n","        #else:\n","          #print(\"Length okay from first e\")\n","\n","      else:\n","        print(\"****Prolem ase \")\n","\n","print(\"Count of K: \",c)\n","\n","print(len(all_pep))\n","\n","print(len(all_pep[100]))\n","print(all_pep[100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CAQb6dqtvKBv"},"source":["##### Remove X from all seq\n","\n","remove_all_X=[]\n","l=len(all_pep[0])\n","print(\"All pep: \",len(all_pep))\n","\n","for i in range(len(all_pep)):\n","  r=all_pep[i]\n","  #print(r)\n","  if r[0]== 'X' or r[window_size-1]== 'X':\n","    remove_all_X.append(r)\n","\n","print(\"With X: \",len(remove_all_X))\n","\n","\n","\n","### For Find all pep without X from All pep\n","after_all_X=[]\n","temp=all_pep[:]\n","count=0\n","for i in range(len(all_pep)):\n","  for j in range(len(remove_all_X)):\n","    if( all_pep[i] == remove_all_X[j] ):\n","      temp.pop(i-count)\n","      count+=1\n","      break\n","after_all_X=temp\n","\n","print(\"Without X: \",len(after_all_X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7TTdzQmvKBy"},"source":["### For Finding Positive Peptides\n","\n","table1=pd.read_csv('table1.csv')\n","\n","positive_seq=[]\n","positive_label=[]\n","\n","\n","seq_id=out.iloc[:,0]    #### out theke value niyar jonno\n","seq_s=out.iloc[:,1]\n","s_len=len(seq_id)\n","\n","\n","\n","tid=table1.iloc[:,0]    #### table1 er value niyar jonno\n","tpos=table1.iloc[:,1]\n","t_len=len(tid)\n","#print(t_len)\n","\n","u=0\n","\n","for i in range(t_len):          #### for table 1\n","    a_id=tid[i]                      ### for store table1 er id\n","    a_pos=tpos[i]                       #### for store table1 er position\n","    #print(\"number: \",i)\n","    for j in range(s_len):\n","        if( a_id == seq_id[j] and a_pos< len(seq_s[j])):      ### For checking PLMD id\n","            start=a_pos-1-1\n","            end=a_pos+half-1\n","            t=seq_s[j]\n","            #print(\"SEQ: \",t)\n","            t=t.rstrip(\"\\n\")\n","            s=len(t)                ### s for akta seq er length\n","            \n","            #print(\"AFTER NEW LINE: \",t)\n","            #print(\"Seq: \",t)\n","            #print(\"Position:\" ,a_pos)\n","            #print(\"Start: \",start)\n","            #print(\"End\",end)\n","            #print(\"ID Match\")\n","\n","            if( start>= half and end <(s-1) ):      ### Sob jaigai e thik ase\n","              l1=a_pos-half-1\n","              w=t[l1:end+1]\n","              #print(\"Seq Sob thik: \",len(w))\n","             #if( w[11] == \"K\" ):\n","              #  print(\"Okay\")\n","             # else:\n","              print(\"Problem of length at sob: \",w)\n","              positive_seq.append(w)\n","              positive_label.append(1)\n","              u+=1\n","            \n","\n","            elif( start>=half and end>=(s-1)):\n","              l1=a_pos-half-1\n","              w=t[l1:s]\n","              s=len(seq_s[j])\n","              print(\"Right problem: \",w)\n","              r=half-(s-a_pos)+1\n","              for h in range(r):\n","                w = w + \"X\"\n","\n","              #if( w[11] == \"K\" ):\n","               # print(\"Okay\")\n","              #else:\n","              print(\"Problem of length at last e: \",w) \n","              positive_seq.append(w)\n","              positive_label.append(1)\n","              #print(\"Seq last e problem:\",w,len(w))\n","              u+=1\n","\n","            \n","            elif(start<half and end<=s):    # First e problem\n","              #print(\"start, end, pos:\",start,end,a_pos)\n","              w=t[0:end+1]\n","              #print(w)\n","              start=half-start\n","              for f in range(start-1):\n","                w=\"X\" + w\n","              print(\"Seq first e problem: \",w)\n","             # if( w[11] == \"K\" ):\n","              #  print(\"Okay\")\n","              #else:\n","               # print(\"Problem of length at first e\")\n","              positive_seq.append(w)\n","              positive_label.append(1)\n","              u+=1\n","\n","\n","\n","            else:\n","              print(\"Problem to find sequence, Len:******* \",s)\n","              print(\"Start, end\", start, end)\n","              print(t,(a_pos))\n","              print(t[a_pos-1])\n","\n","\n","\n","                \n","print(\"Total Match:\",u)   \n","\n","\n","print(len(positive_seq[100]))\n","#print(positive_label)    \n","#print(len(positive_label))    \n","\n","\n","#print(\"Positive Peptides: \",len(positive_seq))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adk1ZiQ9vKB1"},"source":["##### For checking all positive peptides length is equal or not\n","\n","print(\"Total Seq Before Check:\",len(positive_seq))\n","c=0\n","for i in range( len(positive_seq) ):\n","  if( len(positive_seq[i]) == window_size):\n","    #print(positive_seq[i])\n","    #print(i)\n","    c+=1\n","    \n","\n","print(\"Total Count:\",c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I75awcpTvKB4"},"source":["############ Remove X from positive data\n","\n","\n","remove_positive_X=[]\n","l=len(positive_seq[0])\n","print('Length of positive data:',l)\n","\n","print('Number of Positive data: ',len(positive_seq))\n","\n","\n","for i in range(len(positive_seq)):\n","  r=positive_seq[i]\n","  #print(r)\n","  if r[0]== 'X' or r[window_size-1]== 'X':\n","    remove_positive_X.append(r)\n","\n","print('With X: ',len(remove_positive_X))\n","\n","\n","### For Find positive without X from All positive\n","\n","after_positive_X=[]\n","temp1=positive_seq[:]\n","count1=0\n","for i in range(len(positive_seq)):\n","  for j in range(len(remove_positive_X)):\n","    if( positive_seq[i] == remove_positive_X[j] ):\n","      temp1.pop(i-count1)\n","      count1+=1\n","      break\n","after_positive_X=temp1\n","\n","print(\"Without X: \",len(after_positive_X))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nj6SqGBTvKB6"},"source":["### For Finding Negative from All peptides\n","\n","print(\"All peptides: \",len(all_pep))\n","\n","negative_seq=[]\n","temp2=all_pep[:]\n","count2=0\n","for i in range(len(all_pep)):\n","  for j in range(len(positive_seq)):\n","    if( all_pep[i] == positive_seq[j] ):\n","      temp2.pop(i-count2)\n","      count2+=1\n","      break\n","negative_seq=temp2\n","\n","print(\"Negative: \",len(negative_seq))\n","print(\"Positive: \",len(positive_seq))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tSF7QOZvKB8"},"source":["### For Finding Negative without X from All without X\n","\n","after_negative_X=[]\n","temp2=after_all_X[:]\n","count2=0\n","print(\"All: \",len(after_all_X))\n","for i in range(len(after_all_X)):\n","  for j in range(len(after_positive_X)):\n","    if( after_all_X[i] == after_positive_X[j] ):\n","      temp2.pop(i-count2)\n","      count2+=1\n","      break\n","after_negative_X=temp2\n","\n","print(\"With X: \",count2)\n","print(\"Without X: \",len(after_negative_X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9xp4gBrvKB-"},"source":["##### For checking all negative peptides length is equal or not\n","\n","\n","print(\"Total Seq Before Check:\",len(after_negative_X))\n","c=0\n","for i in range( len(after_negative_X) ):\n","  if( len(after_negative_X[i]) == window_size):\n","    #print(after_negative_X[i])\n","    #print(i)\n","    c+=1\n","    \n","\n","print(\"Total Count:\",c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAYmFdr4vKCF"},"source":["######## Make positive labels\n","\n","positive_label=[]\n","for i in range(len(after_positive_X)):\n","    positive_label.append(1)\n","\n","print(\"Positive Length: \",len(positive_label))\n","\n","\n","###convert to csv file for positive data\n","\n","positive_data=pd.DataFrame({'Peptides':after_positive_X,'Labels':positive_label })\n","filename= 'positive_data.csv'\n","positive_data.to_csv(filename,index=False)\n","\n","\n","\n","\n","######## Make negative labels\n","\n","negative_label=[]\n","for i in range(len(after_negative_X)):\n","    negative_label.append(0)\n","\n","print(\"Negative Length: \",len(negative_label))\n","\n","\n","##### Convert to csv file for negative data\n","\n","negative_data=pd.DataFrame({'Peptides':after_negative_X,'Labels':negative_label })\n","filename= 'negative_data.csv'\n","negative_data.to_csv(filename,index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZqKzUXkvKCJ"},"source":["## Convert To fasta for negative seq\n","\n","dataWrite = open(\"negative_seq_23.fasta\",\"w\")\n","l = len(after_negative_X)\n","\n","\n","#print(l)\n","for i in range (l):\n","  dataWrite.write(\">\" + str(i) + \"\\n\" + after_negative_X[i]+\"\\n\")\n","\n","dataWrite.close()\n","#print(dataWrite.read())\n","\n","\n","## Convert To fasta for positive seq\n","\n","dataWrite = open(\"positive_seq_23.fasta\",\"w\")\n","l = len(after_positive_X)\n","\n","i=0\n","\n","#print(l)\n","for i in range (l):\n","  dataWrite.write(\">\" + str(i) + \"\\n\" + after_positive_X[i]+\"\\n\")\n","\n","dataWrite.close()\n","#print(dataWrite.read())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8bK1NfNtFUb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPsJhbcHvKCL"},"source":["#    AAI "]},{"cell_type":"code","metadata":{"id":"Y72FYIG9vKCM"},"source":["##### For Positive data ----- Make AAIndex\n","\n","encode=['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']\n","\n","positive_data= pd.read_csv('/content/drive/My Drive/Conference/positive_data.csv')\n","#print(positive_data)\n","\n","positive_seq=positive_data.iloc[:,0]\n","seq_number=len(positive_seq)\n","seq_len=len(positive_seq[0])\n","\n","\n","print(\"Total Seq: \",seq_number)\n","print(\"Lenght of each sequence: \",seq_len)\n","print(\"Total Encoding: \",len(encode))\n","\n","## Read Dataset\n","df =pd.read_csv(\"/content/drive/My Drive/Conference/AAIndex_28Features_Manual.csv\")\n","t=df.iloc[:,1:]\n","print(\"Before transpose: \",t.shape)\n","\n","\n","aai_positive= np.zeros((seq_number,seq_len,t.shape[0]))\n","print(\"Length of feature vector: \",aai_positive.shape)\n","#print(f_positive)\n","\n","\n","\n","tr=np.transpose(t)       ######## Make transpose matrix\n","#print(tr)\n","#print(tr.iloc[0,:])\n","#print(len(tr.iloc[0,:]))\n","print(\"After transpose: \",tr.shape)\n","\n","\n","\n","for n in range(seq_number):\n","  x1=positive_seq[n]\n","  #print(x1)\n","  for i in range(seq_len):\n","    x=x1[i]\n","    #print(x)\n","    for j in range(len(encode)):\n","      if x==encode[j]:\n","        #print(\"Match\")\n","        aai_positive[n,i,:]=tr.iloc[j,:]\n","\n","print(aai_positive.shape)\n","\n","\n","#print(tr.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1GdIUcBvKCO"},"source":["##### For Negative data ----- Make AAIndex\n","\n","\n","negative_data= pd.read_csv('/content/drive/My Drive/Conference/negative_data.csv')\n","#print(negative_data)\n","negative_seq=negative_data.iloc[:,0]\n","\n","\n","seq_number=len(negative_seq)\n","\n","print(\"Total Seq: \",seq_number)\n","print(\"Lenght of each sequence: \",seq_len)\n","print(\"Total Encoding: \",len(encode))\n","\n","\n","aai_negative= np.zeros((seq_number,seq_len,len(tr.iloc[0,:])))\n","print(\"Length of feature vector: \",aai_negative.shape)\n","#print(f_negative)\n","\n","\n","\n","for n in range(seq_number):\n","  x1=negative_seq.iloc[n]\n","  #print(x1)\n","  for i in range(seq_len):\n","    x=x1[i]\n","    #print(x)\n","    for j in range(len(encode)):\n","      if x==encode[j]:\n","        #print(\"Match\")\n","        aai_negative[n,i,:]=tr.iloc[j,:]\n","\n","print(aai_negative.shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qOPKgDI-vKCR"},"source":["####### Positive data reshape for AAI\n","\n","\n","print(\"Before reshape: \",aai_positive.shape)\n","aai_positive_reshape=np.reshape(aai_positive,(aai_positive.shape[0],aai_positive.shape[1]*aai_positive.shape[2]))\n","\n","print(\"After reshape: \",aai_positive_reshape.shape)\n","\n","\n","\n","####### Negative data reshape for AAI\n","\n","\n","print(\"Before reshape: \",aai_negative.shape)\n","aai_negative_reshape=np.reshape(aai_negative,(aai_negative.shape[0],aai_negative.shape[1]*aai_negative.shape[2]))\n","\n","print(\"After reshape: \",aai_negative_reshape.shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUqqa5O1vKCV"},"source":["# CKSAAP "]},{"cell_type":"code","metadata":{"id":"l-gSwnXavKCV"},"source":["##### For Positive data ----- import CKSAAP where k=5\n","\n","positive_cksaap= pd.read_csv('/content/drive/My Drive/Conference/cksaap_positive_23.csv')\n","\n","print(positive_cksaap.shape)\n","positive_cksaap=positive_cksaap.iloc[:,1:]\n","print(positive_cksaap.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NrUMeVQmvKCY"},"source":["##### For Negative data ----- import CKSAAP where k=5\n","\n","negative_cksaap= pd.read_csv('/content/drive/My Drive/Conference/cksaap_negative_23.csv')\n","\n","print(negative_cksaap.shape)\n","negative_cksaap=negative_cksaap.iloc[:,1:]\n","print(negative_cksaap.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z9aF8TF1vKCZ"},"source":["# AAC "]},{"cell_type":"code","metadata":{"id":"WwWnA8XovKCa"},"source":["##### For Positive data ----- import AAC\n","\n","positive_aac= pd.read_csv('/content/drive/My Drive/Conference/aac_positive_23.csv')\n","\n","print(positive_aac.shape)\n","positive_aac=positive_aac.iloc[:,1:]\n","print(positive_aac.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-K6cyVpvKCc"},"source":["##### For Negative data ----- import AAC\n","\n","negative_aac= pd.read_csv('/content/drive/My Drive/Conference/aac_negative_23.csv')\n","\n","print(negative_aac.shape)\n","negative_aac=negative_aac.iloc[:,1:]\n","print(negative_aac.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0wglR6XvKCe"},"source":["#    Merge AAC, CKSAAP,  AAI for Positive"]},{"cell_type":"code","metadata":{"id":"33VU6bj-vKCf"},"source":["print(positive_cksaap.shape)\n","print(positive_aac.shape)\n","print(aai_positive_reshape.shape)\n","\n","a=positive_cksaap.shape[0]\n","b=positive_cksaap.shape[1] + positive_aac.shape[1] +  aai_positive_reshape.shape[1]\n","\n","positive_feature=np.zeros((a,b))\n","\n","positive_feature=np.append(positive_aac,positive_cksaap, axis=1)\n","print(positive_feature.shape)\n","positive_feature=np.append(positive_feature,aai_positive_reshape, axis=1)\n","print(positive_feature.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"01_JOmr7vKCh"},"source":["#    Merge AAC, CKSAAP,  AAI for Negative"]},{"cell_type":"code","metadata":{"id":"1Hk9olUzvKCh"},"source":["print(negative_cksaap.shape)\n","print(negative_aac.shape)\n","print(aai_negative_reshape.shape)\n","\n","a=negative_cksaap.shape[0]\n","b=negative_cksaap.shape[1] + negative_aac.shape[1] +  aai_negative_reshape.shape[1]\n","\n","negative_feature=np.zeros((a,b))\n","\n","negative_feature=np.append(negative_aac,negative_cksaap, axis=1)\n","print(negative_feature.shape)\n","negative_feature=np.append(negative_feature,aai_negative_reshape, axis=1)\n","print(negative_feature.shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oH5btEP5RI1r"},"source":["####### Feature Scaling\n","\n","print(positive_feature.shape)\n","print(negative_feature.shape)\n","\n","\n","# define min max scaler\n","scaler = MinMaxScaler()\n","\n","# transform data\n","positive_feature_minmax = scaler.fit_transform(positive_feature)\n","\n","# transform data\n","negative_feature_minmax = scaler.fit_transform(negative_feature)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1E5MAzARJGY"},"source":["###### Positive and negative data minmax\n","\n","posi_train_minmax, posi_test_minmax= train_test_split(positive_feature_minmax, test_size=0.2)\n","print('Positive train: ',posi_train_minmax.shape)\n","print('Positive test: ',posi_test_minmax.shape)\n","\n","\n","nega_train_minmax, nega_test_minmax= train_test_split(negative_feature_minmax, test_size=0.2)\n","print('Negative train: ',nega_train_minmax.shape)\n","print('Negative test: ',nega_test_minmax.shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dr17Cbe3lRAR"},"source":["## **Merge Test Data(merge_feature_test, merge_y_test)**\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"z5uRtoXXkR5v"},"source":["####### Merge Test Data(merge_feature_test, merge_y_test)\n","\n","merge_feature_test=np.zeros((len(posi_test_minmax)+len(nega_test_minmax),posi_test_minmax.shape[1]))\n","print(merge_feature_test.shape)\n","\n","merge_feature_test=np.concatenate((posi_test_minmax,nega_test_minmax))\n","print(merge_feature_test.shape)\n","\n","##### label\n","\n","merge_y_test=[]\n","\n","for i in range(len(posi_test_minmax)):     #### for positive\n","    merge_y_test.append(1)\n","\n","    \n","for j in range(len(nega_test_minmax)):    #### for negative\n","    merge_y_test.append(0)\n","\n","print(len(merge_y_test))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hvq2mx4WllnV"},"source":["#**Auto Encoder**"]},{"cell_type":"code","metadata":{"id":"aR0AVX0vRJO6"},"source":["########## Imbalance Solve By Autoecoder\n","\n","from keras.models import Model\n","from keras.layers import  Input, Dense\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(\"Starting Auto Encoder.............\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5suqpA1csUFX"},"source":["####### Variable Set\n","\n","ratio=len(nega_train_minmax)/len(posi_train_minmax)\n","print(\"Negative To Positive Ratio: \",ratio)\n","\n","input_size=posi_train_minmax.shape[0]\n","feature_size=posi_train_minmax.shape[1]\n","print(\"Positive Train Input Size: \",input_size)\n","print(\"Feature Size: \",feature_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIvZoKIwRI-o"},"source":["#### Autoencoder Model\n","\n","encoding_dim=64\n","\n","input_data=Input(shape=(feature_size,))\n","encoded=Dense(encoding_dim, activation='relu') (input_data)\n","decoded=Dense(feature_size,activation='sigmoid') (encoded)\n","\n","autoencoder= Model(input_data, decoded)\n","\n","encoder= Model(input_data, encoded)\n","\n","encoded_input=Input(shape=(encoding_dim,))\n","\n","decoder_layer= autoencoder.layers[-1]\n","\n","decoder= Model(encoded_input, decoder_layer(encoded_input))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fdrs9895RIov"},"source":["### Compile and train the model\n","\n","autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n","autoencoder.fit(posi_train_minmax,posi_train_minmax,epochs=50, batch_size=256, shuffle=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6O2YbfOQrcxZ"},"source":["#### Predict with negative(majority class)\n","\n","encoded_input=encoder.predict(nega_train_minmax)\n","decoded_input= decoder.predict(encoded_input)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHBDvrlC3056"},"source":["#### Predict with positive(minority class)\n","\n","encoded_input_posi=encoder.predict(posi_train_minmax)\n","decoded_input_posi= decoder.predict(encoded_input)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ehie39ErdDc"},"source":["##### Calculate Distance For NUS-1\n","'''\n","import math\n","\n","distance_list=[]\n","\n","for i in range(nega_train_minmax.shape[0]):\n","  t=sum(pow(nega_train_minmax[i]-decoded_input[i],2))\n","  distance_list.append(t)\n","\n","#print(distance_list)\n","\n","sort_index=np.argsort(distance_list)[::-1] #### Decending Order Sort\n","#print(sort_index)\n","\n","# Choose index according to sorting\n","fix_index=sort_index[0:input_size]\n","print(fix_index)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlp3M0okxfNz"},"source":["##### Calculate Distance for Majority class(Negative)\n","\n","import math as m\n","\n","distance_list_majority=[]\n","\n","for i in range(nega_train_minmax.shape[0]):\n","  t=m.fsum(pow(nega_train_minmax[i]-decoded_input[i],2))\n","  distance_list_majority.append(t)\n","\n","print(len(distance_list_majority))\n","\n","sort_index_majority=np.argsort(distance_list_majority)[::-1] #### Decending Order Sort\n","#print(sort_index)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uW7Mk7Z4xfQ_"},"source":["##### Calculate Distance For Minority class(Positive)\n","\n","import math\n","\n","distance_list_minority=[]\n","\n","for i in range(posi_train_minmax.shape[0]):\n","  t=m.fsum(pow(posi_train_minmax[i]-decoded_input_posi[i],2))\n","  distance_list_minority.append(t)\n","\n","#print(distance_list)\n","\n","sort_index_minority=np.argsort(distance_list_minority)[::-1] #### Decending Order Sort\n","#print(sort_index)\n","\n","h=int(np.ceil(len(sort_index_minority)/2))\n","print(h)\n","\n","s=0\n","for i in range(h):\n","  j=sort_index_minority[i]\n","  s=s+distance_list_minority[j]\n","\n","last_mid_avrg=s/h\n","print(\"Last Mid Average:\",last_mid_avrg)\n","e=sort_index_minority[0]\n","max_dis=distance_list_minority[e]\n","print(\"Max Distance:\",max_dis)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGPIzozfxfVr"},"source":["##### Select Index for Majority\n","\n","select_index_majority=[]\n","\n","for j in sort_index_majority:\n","  d=distance_list_majority[j]\n","  if d > max_dis or d > last_mid_avrg:\n","    select_index_majority.append(j)\n","\n","print(len(select_index_majority))\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUhtvqHLrc6U"},"source":["########### Select negative data\n","\n","s=len(select_index_majority)\n","e=np.zeros((s,feature_size))\n","e=nega_train_minmax\n","final_nega_train=np.zeros((s,feature_size))\n","\n","j=0\n","for i in select_index_majority:\n","  final_nega_train[j]=e[i]\n","  j=j+1\n","  \n","print(\"Final Negative Data Size:\",final_nega_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAvdXErXrcuE"},"source":["print(final_nega_train.shape)\n","print(posi_train_minmax.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvi6E849ymlV"},"source":["##### Error Find\n","\n","print(select_index_majority[0])\n","a=final_nega_train[0]-nega_train_minmax[22886]\n","b=m.fsum(a)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1SI2UKpvKCk"},"source":["# Combine Positive & Negative  (merge_feature, merge_y)"]},{"cell_type":"code","metadata":{"id":"QipnZ1EQvKCl"},"source":["#   Merge Train Data\n","\n","merge_feature=np.zeros((len(posi_train_minmax)+len(final_nega_train),posi_train_minmax.shape[1]))\n","print(merge_feature.shape)\n","\n","merge_feature=np.concatenate((posi_train_minmax,final_nega_train))\n","print(merge_feature.shape)\n","\n","##### label\n","\n","merge_y=[]\n","\n","for i in range(len(posi_train_minmax)):     #### for positive\n","    merge_y.append(1)\n","\n","    \n","for j in range(len(final_nega_train)):    #### for negative\n","    merge_y.append(0)\n","\n","print(len(merge_y))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-6afjB5LLlE"},"source":["print('Original dataset shape {}'.format(Counter(merge_y)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeGVS3t6vKCn"},"source":["# Feature Selection (merge_feature_best, merge_y) "]},{"cell_type":"code","metadata":{"id":"FThCZ0m2vKCn"},"source":["from sklearn.feature_selection import SelectKBest, chi2, f_regression\n","\n","feature_selection_model=SelectKBest(score_func=f_regression,k=500)\n","\n","merge_feature_best = feature_selection_model.fit_transform(merge_feature, merge_y)\n","\n","print(merge_feature_best.shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkRvG0SHvKCq"},"source":["print('Original dataset shape {}'.format(Counter(merge_y)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJsDec0RWqqQ"},"source":["### Feature Index Find For Test Set\n","\n","select_feature_index_score=feature_selection_model.scores_\n","print(select_feature_index_score)\n","\n","sort_index_feature=np.argsort(select_feature_index_score)[::-1] #### Decending Order Sort\n","print(sort_index_feature)\n","\n","sort_index_feature=sort_index_feature[0:500]\n","print(len(sort_index_feature))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1v1fmRQWqT2"},"source":["#### Select the sorted feature for test set\n","\n","f_test=np.zeros((12388,500))\n","print(f_test.shape)\n","print(len(sort_index_feature))\n","\n","j=0\n","for i in sort_index_feature:\n","  f_test[:,j]=merge_feature_test[:,i]\n","  j=j+1\n","\n","#print(f_test[0,:])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZGevLqevKCs"},"source":["############## Split positive & negative\n","\"\"\"\n","positive=np.zeros((3107,500))\n","print(positive.shape)\n","positive_y=[]\n","\n","negative=np.zeros((58827,500))\n","print(negative.shape)\n","negative_y=[]\n","j=0\n","k=0\n","\n","for i in range(len(merge_y)):\n","    if (merge_y[i]==1):\n","        t=merge_feature_best[i]\n","        positive[j,:] = t\n","        positive_y.append(1)\n","    elif (merge_y[i]==0):\n","        r=merge_feature_best[i]\n","        negative[k,:]  = r\n","        negative_y.append(0)\n","\n","print(len(positive))\n","\n","print(len(negative))\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8y7eCTbvKCt"},"source":["######## Split test and train\n","\"\"\"\n","\n","positive_train_set,positive_test_independent,y_positive_train_set,y_positive_test_independent=train_test_split(positive, positive_y,test_size=0.2)\n","\n","print(len(positive_train_set))\n","print(len(positive_test_independent))\n","\n","\n","negative_train_set,negative_test_independent,y_negative_train_set,y_negative_test_independent=train_test_split(negative, negative_y,test_size=0.010565)\n","\n","print(len(negative_train_set))\n","print(len(negative_test_independent))\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnvD6vlUvKCv"},"source":["######## Train\n","\"\"\"\n","merge_train=np.zeros((len(positive_train_set)+len(negative_train_set),positive_train_set.shape[1]))\n","print(merge_train.shape)\n","\n","merge_train=np.concatenate((positive_train_set,negative_train_set))\n","print(merge_train.shape)\n","\n","##### label\n","\n","merge_train_y=[]\n","\n","for i in range(len(positive_train_set)):     #### for positive\n","    merge_train_y.append(1)\n","\n","    \n","for j in range(len(negative_train_set)):    #### for negative\n","    merge_train_y.append(0)\n","\n","print(len(merge_train_y))\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MR1V5J8VvKCx"},"source":["######## Test\n","\"\"\"\n","\n","merge_test=np.zeros((len(positive_test_independent)+len(negative_test_independent),positive_test_independent.shape[1]))\n","print(merge_test.shape)\n","\n","merge_test=np.concatenate((positive_test_independent,negative_test_independent))\n","print(merge_test.shape)\n","\n","##### label\n","\n","merge_test_y=[]\n","\n","for i in range(len(positive_test_independent)):     #### for positive\n","    merge_test_y.append(1)\n","\n","    \n","for j in range(len(negative_test_independent)):    #### for negative\n","    merge_test_y.append(0)\n","\n","print(len(merge_test_y))\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xjD8SrYTvKC6"},"source":["# To Solve Imbalance Problem With Central Based Undersampling"]},{"cell_type":"code","metadata":{"id":"0QHNBS9MvKC6"},"source":["print('Original dataset shape {}'.format(Counter(merge_train_y)))\n","cc = ClusterCentroids(random_state=42)\n","X_res, y_res = cc.fit_sample(merge_train, merge_train_y)\n","print('Resampled dataset shape {}'.format(Counter(y_res)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jd_ByAVvvKC8"},"source":["# Data Split Into Test & Train"]},{"cell_type":"code","metadata":{"id":"WQ3LKPSOvKC8"},"source":["x_train_set,x_test_independent,y_train_set,y_test_independent=train_test_split(X_res, y_res,test_size=0.2)\n","print(\"Test Size: \",len(x_test_independent))\n","print('In Test: {}'.format(Counter(y_test_independent)))\n","print(\"Train Size: \",len(x_train_set))\n","print('In Test: {}'.format(Counter(y_train_set)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUX4dHEtvKC_"},"source":["# Scaling Train & Test data"]},{"cell_type":"code","metadata":{"id":"rtHyWjztvKC_"},"source":["\"\"\"\n","# define min max scaler\n","scaler = MinMaxScaler()\n","\n","###### Train\n","# transform data\n","x_train_minmax = scaler.fit_transform(X_res)\n","#print(x_train_minmax)\n","\n","\n","\n","##### Test\n","# transform data\n","x_test_minmax = scaler.fit_transform(merge_test)\n","#print(x_test_minmax)\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_86CZSevKDE"},"source":["###### Remove some errors in jupyter\n","\"\"\"\n","merge_y_test=np.array(merge_y_test)\n","print(merge_y_test.dtype)\n","\n","y_train_set=np.array(y_res)\n","print(y_train_set.dtype)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeI1ND4TH7ma"},"source":["###### Remove some errors in jupyter\n","\"\"\"\n","merge_y=np.array(merge_y)\n","print(merge_y.dtype)\n","\n","merge_y_test=np.array(merge_y_test)\n","print(merge_y_test.dtype)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCiAYOQ8vKDH"},"source":["# Support Vector Machine"]},{"cell_type":"code","metadata":{"id":"pzxAL2VD1a6F"},"source":["###### Remove some errors in jupyter\n","\n","merge_y=np.array(merge_y)\n","print(merge_y.dtype)\n","\n","merge_y_test=np.array(merge_y_test)\n","print(merge_y_test.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"68iGbf1pvKDI"},"source":["################## KFold Cross Validation approach With NUS-2\n","\n","\n","clf = SVC(C=16)\n","kf = KFold(n_splits=10,shuffle=True)\n","kf.split(merge_feature_best)    \n","     \n","# Initialize the accuracy of the models to blank list. The accuracy of each model will be appended to this list\n","accuracy_model = []\n","sn_model = []\n","sp_model = []\n","f1_model = []\n","mcc_model= []\n","auc_model= []\n","\n","\n","print(\"************************ Support Vector Machine ***********************\")\n"," \n","# Iterate over each train-test split\n","for train_index, test_index in kf.split(merge_feature_best):\n","    \n","    # Split train-test\n","    X_train = merge_feature_best[train_index] \n","    X_test = merge_feature_best[test_index]\n","    y_train = merge_y[train_index]\n","    y_test =  merge_y[test_index]\n","    \n","    # Train the model\n","    model = clf.fit(X_train, y_train)\n","    \n","    # Append to accuracy_model the accuracy of the model\n","    y_pred=model.predict(X_test)\n","    #y_pred_prob= model.predict_prob(X_test)\n","    accuracy_model.append(accuracy_score(y_test, y_pred))\n","    \n","    confusion=confusion_matrix(y_test,y_pred)\n","    print(\"Confusion Matrix: \",confusion)\n","    TP=confusion[1,1]\n","    print(\"TP:\",TP)\n","    TN=confusion[0,0]\n","    print(\"TN: \",TN)\n","    FP=confusion[0,1]\n","    print(\"FP: \",FP)\n","    FN=confusion[1,0]\n","    print(\"FN: \",FN)\n","    \n","    SN=TP/(float)(TP+FN)\n","    sn_model.append(SN)\n","    print(\"Sensivity: \",SN)\n","    \n","    SP=TN/(float)(TN+FP)\n","    sp_model.append(SP)\n","    print(\"Specificity: \",SP)\n","    \n","    mcc= matthews_corrcoef(y_test,y_pred)\n","    mcc_model.append(mcc)\n","    print(\"MCC: \",mcc)\n","    \n","    a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","    print(\"Accuracy from quation: \",a1)\n","    \n","    f1_model.append(f1_score(y_test, y_pred))\n","    \n","    auc=roc_auc_score(y_test,y_pred)\n","    auc_model.append(auc)\n","    print(\"AUC: \",auc)\n","    \n","    print(\"**************************************************\")\n","\n","\n","print(\"************************ 10-Fold Cross Validation ***********************\")\n","\n","######Print the accuracy and others metrics \n","print(\"Accuracy:\",accuracy_model)\n","print(\"Sensitivity:\",sn_model)\n","print(\"Specificity:\",sp_model)\n","print(\"F-1 Score:\",f1_model)\n","print(\"MCC Score:\",mcc_model)\n","print(\"AUC Score:\",auc_model)\n","\n","\n","\n","a=m.fsum(accuracy_model)/len(accuracy_model)\n","print(\"Average Accuracy: \",a,\" \",np.std(accuracy_model))\n","\n","b=m.fsum(sn_model)/len(accuracy_model)\n","print(\"Average Sensitivity: \",b,\" \",np.std(sn_model))\n","\n","c=m.fsum(sp_model)/len(accuracy_model)\n","print(\"Average Specificity: \",c,\" \",np.std(sp_model))\n","\n","d=m.fsum(f1_model)/len(accuracy_model)\n","print(\"Average f-1 Score: \",d)\n","\n","e=m.fsum(mcc_model)/len(accuracy_model)\n","print(\"Average MCC Score: \",e,\" \",np.std(mcc_model))\n","\n","f=m.fsum(auc_model)/len(auc_model)\n","print(\"Average AUC Score: \",f,\" \",np.std(auc_model))\n","\n","\n","print(\"************************ Independent Test Set ***********************\")\n","\n","# Train the model\n","#model = clf.fit(merge_feature_best, merge_y)\n","\n","# Append to accuracy_model the accuracy of the model\n","y_pred=model.predict(f_test)\n","\n","confusion=confusion_matrix(merge_y_test,y_pred)\n","print(\"Confusion Matrix: \",confusion)\n","TP=confusion[1,1]\n","print(\"TP:\",TP)\n","TN=confusion[0,0]\n","print(\"TN: \",TN)\n","FP=confusion[0,1]\n","print(\"FP: \",FP)\n","FN=confusion[1,0]\n","print(\"FN: \",FN)\n","\n","SN=TP/(float)(TP+FN)\n","sn_model.append(SN)\n","print(\"Sensivity: \",SN)\n","    \n","SP=TN/(float)(TN+FP)\n","sp_model.append(SP)\n","print(\"Specificity: \",SP)\n","    \n","mcc= matthews_corrcoef(merge_y_test,y_pred)\n","mcc_model.append(mcc)\n","print(\"MCC: \",mcc)\n","    \n","a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","print(\"Accuracy from quation: \",a1)\n","    \n","auc=roc_auc_score(merge_y_test,y_pred)\n","auc_model.append(auc)\n","print(\"AUC: \",auc)\n","    \n","print(\"**************************************************\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWJz9dxovKDL"},"source":["# XG Boost"]},{"cell_type":"code","metadata":{"id":"c3a_hSYTvKDM"},"source":["################## KFold Cross Validation approach With Center based\n","\n","from xgboost import XGBClassifier\n","import xgboost as xgb\n","\n","clf = XGBClassifier()\n","kf = KFold(n_splits=10,shuffle=True)\n","kf.split(merge_feature_best)    \n","     \n","# Initialize the accuracy of the models to blank list. The accuracy of each model will be appended to this list\n","accuracy_model = []\n","sn_model = []\n","sp_model = []\n","f1_model = []\n","mcc_model= []\n","auc_model= []\n","\n","\n","print(\"************************ XG Boost ***********************\")\n"," \n","# Iterate over each train-test split\n","for train_index, test_index in kf.split(merge_feature_best):\n","    \n","    # Split train-test\n","    X_train = merge_feature_best[train_index] \n","    X_test = merge_feature_best[test_index]\n","    y_train = merge_y[train_index]\n","    y_test =  merge_y[test_index]\n","    \n","    # Train the model\n","    model = clf.fit(X_train, y_train)\n","    \n","    # Append to accuracy_model the accuracy of the model\n","    y_pred=model.predict(X_test)\n","    #y_pred_prob= model.predict_prob(X_test)\n","    accuracy_model.append(accuracy_score(y_test, y_pred)*100)\n","    \n","    confusion=confusion_matrix(y_test,y_pred)\n","    print(\"Confusion Matrix: \",confusion)\n","    TP=confusion[1,1]\n","    print(\"TP:\",TP)\n","    TN=confusion[0,0]\n","    print(\"TN: \",TN)\n","    FP=confusion[0,1]\n","    print(\"FP: \",FP)\n","    FN=confusion[1,0]\n","    print(\"FN: \",FN)\n","    \n","    SN=TP/(float)(TP+FN)\n","    sn_model.append(SN)\n","    print(\"Sensivity: \",SN)\n","    \n","    SP=TN/(float)(TN+FP)\n","    sp_model.append(SP)\n","    print(\"Specificity: \",SP)\n","    \n","    mcc= matthews_corrcoef(y_test,y_pred)\n","    mcc_model.append(mcc)\n","    print(\"MCC: \",mcc)\n","    \n","    a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","    print(\"Accuracy from quation: \",a1)\n","    \n","    f1_model.append(f1_score(y_test, y_pred))\n","    \n","    auc=roc_auc_score(y_test,y_pred)\n","    auc_model.append(auc)\n","    print(\"AUC: \",auc)\n","    \n","    print(\"**************************************************\")\n","\n","\n","print(\"************************ 10-Fold Cross Validation ***********************\")\n","\n","######Print the accuracy and others metrics \n","print(\"Accuracy:\",accuracy_model)\n","print(\"Sensitivity:\",sn_model)\n","print(\"Specificity:\",sp_model)\n","print(\"F-1 Score:\",f1_model)\n","print(\"MCC Score:\",mcc_model)\n","print(\"AUC Score:\",auc_model)\n","\n","\n","\n","a=sum(accuracy_model)/len(accuracy_model)\n","print(\"Average Accuracy: \",a,\" \",np.std(accuracy_model))\n","\n","b=sum(sn_model)/len(accuracy_model)\n","print(\"Average Sensitivity: \",b,\" \",np.std(sn_model))\n","\n","c=sum(sp_model)/len(accuracy_model)\n","print(\"Average Specificity: \",c,\" \",np.std(sp_model))\n","\n","d=sum(f1_model)/len(accuracy_model)\n","print(\"Average f-1 Score: \",d)\n","\n","e=sum(mcc_model)/len(accuracy_model)\n","print(\"Average MCC Score: \",e,\" \",np.std(mcc_model))\n","\n","f=sum(auc_model)/len(auc_model)\n","print(\"Average AUC Score: \",f,\" \",np.std(auc_model))\n","\n","\n","\n","print(\"************************ Independent Test Set ***********************\")\n","\n","# Train the model\n","#model = clf.fit(x_train_minmax, y_train_set)\n","\n","# Append to accuracy_model the accuracy of the model\n","y_pred=model.predict(f_test)\n","\n","\n","confusion=confusion_matrix(merge_y_test,y_pred)\n","print(\"Confusion Matrix: \",confusion)\n","TP=confusion[1,1]\n","print(\"TP:\",TP)\n","TN=confusion[0,0]\n","print(\"TN: \",TN)\n","FP=confusion[0,1]\n","print(\"FP: \",FP)\n","FN=confusion[1,0]\n","print(\"FN: \",FN)\n","\n","SN=TP/(float)(TP+FN)\n","sn_model.append(SN)\n","print(\"Sensivity: \",SN)\n","    \n","SP=TN/(float)(TN+FP)\n","sp_model.append(SP)\n","print(\"Specificity: \",SP)\n","    \n","mcc= matthews_corrcoef(merge_y_test,y_pred)\n","mcc_model.append(mcc)\n","print(\"MCC: \",mcc)\n","    \n","a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","print(\"Accuracy from quation: \",a1)\n","    \n","    \n","auc=roc_auc_score(merge_y_test,y_pred)\n","auc_model.append(auc)\n","print(\"AUC: \",auc)\n","    \n","print(\"**************************************************\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xt8Ntri6vKDN"},"source":["# Random Forest "]},{"cell_type":"code","metadata":{"id":"wWexQvOFvKDO"},"source":["################## KFold Cross Validation approach With Center based\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","clf = RandomForestClassifier(max_depth=2, random_state=0)\n","kf = KFold(n_splits=10,shuffle=True)\n","kf.split(merge_feature_best)    \n","     \n","# Initialize the accuracy of the models to blank list. The accuracy of each model will be appended to this list\n","accuracy_model = []\n","sn_model = []\n","sp_model = []\n","f1_model = []\n","mcc_model= []\n","auc_model= []\n","\n"," \n","print(\"************************ Random Forest ***********************\")    \n","    \n","# Iterate over each train-test split\n","for train_index, test_index in kf.split(merge_feature_best):\n","    \n","    # Split train-test\n","    X_train = merge_feature_best[train_index] \n","    X_test = merge_feature_best[test_index]\n","    y_train = merge_y[train_index]\n","    y_test =  merge_y[test_index]\n","    \n","    # Train the model\n","    model = clf.fit(X_train, y_train)\n","    \n","    # Append to accuracy_model the accuracy of the model\n","    y_pred=model.predict(X_test)\n","    #y_pred_prob= model.predict_prob(X_test)\n","    accuracy_model.append(accuracy_score(y_test, y_pred)*100)\n","    \n","    confusion=confusion_matrix(y_test,y_pred)\n","    print(\"Confusion Matrix: \",confusion)\n","    TP=confusion[1,1]\n","    print(\"TP:\",TP)\n","    TN=confusion[0,0]\n","    print(\"TN: \",TN)\n","    FP=confusion[0,1]\n","    print(\"FP: \",FP)\n","    FN=confusion[1,0]\n","    print(\"FN: \",FN)\n","    \n","    SN=TP/(float)(TP+FN)\n","    sn_model.append(SN)\n","    print(\"Sensivity: \",SN)\n","    \n","    SP=TN/(float)(TN+FP)\n","    sp_model.append(SP)\n","    print(\"Specificity: \",SP)\n","    \n","    mcc= matthews_corrcoef(y_test,y_pred)\n","    mcc_model.append(mcc)\n","    print(\"MCC: \",mcc)\n","    \n","    a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","    print(\"Accuracy from quation: \",a1)\n","    \n","    f1_model.append(f1_score(y_test, y_pred))\n","    \n","    auc=roc_auc_score(y_test,y_pred)\n","    auc_model.append(auc)\n","    print(\"AUC: \",auc)\n","    \n","    print(\"**************************************************\")\n","\n","\n","print(\"************************ 10-Fold Cross Validation ***********************\")\n","\n","######Print the accuracy and others metrics \n","print(\"Accuracy:\",accuracy_model)\n","print(\"Sensitivity:\",sn_model)\n","print(\"Specificity:\",sp_model)\n","print(\"F-1 Score:\",f1_model)\n","print(\"MCC Score:\",mcc_model)\n","print(\"AUC Score:\",auc_model)\n","\n","\n","\n","a=sum(accuracy_model)/len(accuracy_model)\n","print(\"Average Accuracy: \",a,\" \",np.std(accuracy_model))\n","\n","b=sum(sn_model)/len(accuracy_model)\n","print(\"Average Sensitivity: \",b,\" \",np.std(sn_model))\n","\n","c=sum(sp_model)/len(accuracy_model)\n","print(\"Average Specificity: \",c,\" \",np.std(sp_model))\n","\n","d=sum(f1_model)/len(accuracy_model)\n","print(\"Average f-1 Score: \",d)\n","\n","e=sum(mcc_model)/len(accuracy_model)\n","print(\"Average MCC Score: \",e,\" \",np.std(mcc_model))\n","\n","f=sum(auc_model)/len(auc_model)\n","print(\"Average AUC Score: \",f,\" \",np.std(auc_model))\n","\n","\n","\n","print(\"************************ Independent Test Set ***********************\")\n","\n","# Train the model\n","#model = clf.fit(x_train_minmax, y_train_set)\n","\n","# Append to accuracy_model the accuracy of the model\n","y_pred=model.predict(f_test)\n","\n","confusion=confusion_matrix(merge_y_test,y_pred)\n","print(\"Confusion Matrix: \",confusion)\n","TP=confusion[1,1]\n","print(\"TP:\",TP)\n","TN=confusion[0,0]\n","print(\"TN: \",TN)\n","FP=confusion[0,1]\n","print(\"FP: \",FP)\n","FN=confusion[1,0]\n","print(\"FN: \",FN)\n","\n","SN=TP/(float)(TP+FN)\n","sn_model.append(SN)\n","print(\"Sensivity: \",SN)\n","    \n","SP=TN/(float)(TN+FP)\n","sp_model.append(SP)\n","print(\"Specificity: \",SP)\n","    \n","mcc= matthews_corrcoef(merge_y_test,y_pred)\n","mcc_model.append(mcc)\n","print(\"MCC: \",mcc)\n","    \n","a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","print(\"Accuracy from quation: \",a1)\n","\n","  \n","auc=roc_auc_score(merge_y_test,y_pred)\n","auc_model.append(auc)\n","print(\"AUC: \",auc)\n","    \n","print(\"**************************************************\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-Y87jWRvKDQ"},"source":["# Light GBM"]},{"cell_type":"code","metadata":{"id":"1AOQq-6wvKDR"},"source":["################## KFold Cross Validation approach With Center based\n","\n","import lightgbm as lgb\n","\n","#Specifying the parameter\n","params={}\n","params['learning_rate']=0.03\n","params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n","params['objective']='binary' #Binary target feature\n","params['metric']='binary_logloss' #metric for binary classification\n","params['max_depth']=10\n","\n","\n","kf = KFold(n_splits=10,shuffle=True)\n","kf.split(merge_feature_best)    \n","     \n","# Initialize the accuracy of the models to blank list. The accuracy of each model will be appended to this list\n","accuracy_model = []\n","sn_model = []\n","sp_model = []\n","f1_model = []\n","mcc_model= []\n","auc_model= []\n","\n","\n","print(\"********************** Light GBM************************\")\n"," \n","# Iterate over each train-test split\n","for train_index, test_index in kf.split(merge_feature_best):\n","    \n","    # Split train-test\n","    X_train = merge_feature_best[train_index] \n","    X_test = merge_feature_best[test_index]\n","    y_train = merge_y[train_index]\n","    y_test =  merge_y[test_index]\n","    \n","    #converting the dataset into proper LGB format \n","    d_train=lgb.Dataset(X_train, label=y_train)\n","\n","    #train the model \n","    clf=lgb.train(params,d_train)\n","    \n","    # Append to accuracy_model the accuracy of the model\n","    y_pred=model.predict(X_test)\n","    accuracy_model.append(accuracy_score(y_test, y_pred)*100)\n","    \n","    confusion=confusion_matrix(y_test,y_pred)\n","    print(\"Confusion Matrix: \",confusion)\n","    TP=confusion[1,1]\n","    print(\"TP:\",TP)\n","    TN=confusion[0,0]\n","    print(\"TN: \",TN)\n","    FP=confusion[0,1]\n","    print(\"FP: \",FP)\n","    FN=confusion[1,0]\n","    print(\"FN: \",FN)\n","    \n","    SN=TP/(float)(TP+FN)\n","    sn_model.append(SN)\n","    print(\"Sensivity: \",SN)\n","    \n","    SP=TN/(float)(TN+FP)\n","    sp_model.append(SP)\n","    print(\"Specificity: \",SP)\n","    \n","    mcc= matthews_corrcoef(y_test,y_pred)\n","    mcc_model.append(mcc)\n","    print(\"MCC: \",mcc)\n","    \n","    a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","    print(\"Accuracy from quation: \",a1)\n","    \n","    f1_model.append(f1_score(y_test, y_pred))\n","    \n","    auc=roc_auc_score(y_test,y_pred)\n","    auc_model.append(auc)\n","    print(\"AUC: \",auc)\n","    \n","    print(\"**************************************************\")\n","\n","\n","print(\"************************ 10-Fold Cross Validation ***********************\")\n","\n","######Print the accuracy and others metrics \n","print(\"Accuracy:\",accuracy_model)\n","print(\"Sensitivity:\",sn_model)\n","print(\"Specificity:\",sp_model)\n","print(\"F-1 Score:\",f1_model)\n","print(\"MCC Score:\",mcc_model)\n","print(\"AUC Score:\",auc_model)\n","\n","\n","\n","a=sum(accuracy_model)/len(accuracy_model)\n","print(\"Average Accuracy: \",a,\" \",np.std(accuracy_model))\n","\n","b=sum(sn_model)/len(accuracy_model)\n","print(\"Average Sensitivity: \",b,\" \",np.std(sn_model))\n","\n","c=sum(sp_model)/len(accuracy_model)\n","print(\"Average Specificity: \",c,\" \",np.std(sp_model))\n","\n","d=sum(f1_model)/len(accuracy_model)\n","print(\"Average f-1 Score: \",d)\n","\n","e=sum(mcc_model)/len(accuracy_model)\n","print(\"Average MCC Score: \",e,\" \",np.std(mcc_model))\n","\n","f=sum(auc_model)/len(auc_model)\n","print(\"Average AUC Score: \",f,\" \",np.std(auc_model))\n","\n","\n","\n","print(\"************************ Independent Test Set ***********************\")\n","\n","#converting the dataset into proper LGB format \n","d_train=lgb.Dataset(merge_feature_best, label=merge_y)\n","\n","#Specifying the parameter\n","params={}\n","params['learning_rate']=0.03\n","params['boosting_type']='gbdt'       #GradientBoostingDecisionTree\n","params['objective']='binary'         #Binary target feature\n","params['metric']='binary_logloss'    #metric for binary classification\n","params['max_depth']=10\n","\n","\n","#train the model \n","clf=lgb.train(params,d_train)\n","\n","# Append to accuracy_model the accuracy of the model\n","y_pred=model.predict(f_test)\n","\n","confusion=confusion_matrix(merge_y_test,y_pred)\n","print(\"Confusion Matrix: \",confusion)\n","TP=confusion[1,1]\n","print(\"TP:\",TP)\n","TN=confusion[0,0]\n","print(\"TN: \",TN)\n","FP=confusion[0,1]\n","print(\"FP: \",FP)\n","FN=confusion[1,0]\n","print(\"FN: \",FN)\n","\n","SN=TP/(float)(TP+FN)\n","sn_model.append(SN)\n","print(\"Sensivity: \",SN)\n","    \n","SP=TN/(float)(TN+FP)\n","sp_model.append(SP)\n","print(\"Specificity: \",SP)\n","    \n","mcc= matthews_corrcoef(merge_y_test,y_pred)\n","mcc_model.append(mcc)\n","print(\"MCC: \",mcc)\n","    \n","a1=(TN+TP)/(float)(TN+TP+FN+FP)\n","print(\"Accuracy from quation: \",a1)\n","    \n","auc=roc_auc_score(merge_y_test,y_pred)\n","auc_model.append(auc)\n","print(\"AUC: \",auc)\n","    \n","print(\"**************************************************\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQBYB1WUvKDU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"En8BoheDvKDX"},"source":[""],"execution_count":null,"outputs":[]}]}